---
title: Reflection in Rust with procedural macros
description: A tutorial on building and deploying an interactive bot in Rust with Serenity & shuttle
author: ben
tags: [rust, tutorial]
thumb: proc-macro-banner.png
cover: proc-macro-banner.png
date: "2022-12-23T15:00:00"
---


# Reflection in Rust with procedural macros
This is the probably the more complex but powerful parts of Rust. It is one of the biggest differentiators of Rust compared to other languages for me. If you have ever seen this syntax and left scratching your head, then this post is for you
```rust
#[derive(derive_macros::MyTrait)] // << ðŸ¤¨
struct X {}
```
This article will cover the concept and some use cases, so you don't need to know a lot of Rust. However the example section assumes you have written some Rust (`if let`, `struct` , `trait` etc).

In this post we will compare how Rust's compile time, token based approach to object reflection is different to the approach in JavaScript's runtime approach to reflection. 
## What are Rust macros
Macros are a way of generating Rust code. They use tokens which are small sections of syntax / grouped characters. Keywords, identifiers and operators are examples can be considered as tokens. Token streams are vectors/ordered collections of tokens. In Rust some tokens are grouped and thus the stream is not flat. Macros take a input token stream and *output* another token stream. Macros are *expanded* at compile time so the output is checked syntactically and by types. They are very powerful, so it is careful to use the in an intuitive way for programs to still be understandable and maintainable.

Rust offers a `macro_rules!` for creating macros using a pattern syntax Rust has designed. These are current limited to just expression and statement invocations using `my_macro!` syntax. An example of an expression based macro is `println!`. Designing a function to print results to the terminal difficult to get design well just using a function. Instead it is implemented as a macro, this allows writing a formatting string that interpolates variables in the scope (e.g. `println!("{my_var}");`). Also as the input for macros is just a token stream, it is up to the macro to decide what commas mean and so `println!` arguments act *variadic-ly*.

`macro_rules!` are easier to get started with as they can written and used anywhere inside the same crate. However as will see they only work for user token inputs (not on existing items) and their pattern syntax is limited. In this article we will look focus only on the more advanced procedural macros.
### Procedural macros
Compared to `macro_rules!` procedural macros are much more powerful in that they process the token stream using Rust code:
```rust
#[proc_macro]
pub fn my_macro(input: TokenStream) -> TokenStream {
	input
}
```
Procedural macros are different to `macro_rules!` in that they can **additionally** work on the tokens of existing structures. This includes `fn` ,`trait` , `struct` and `enum` declarations.

They are a little bit more complicated than `macro_rules!`. They require creating a separate crate for the function. This article will walk through all the steps and file structure required to add a proc macro to a crate. The name "procedural" is often shortened to just "proc".
## Runtime reflection and JavaScript
Before we start with writing procedural macros look at what reflection is and how it is implemented in the JavaScript language. Reflection is looking at the properties a program. There are various points to introspect such as the name of declarations, the structure of fields. In the following example we will be looking at the fields of a *object*.

JavaScript objects can be inspected at runtime. There are no fixed structures in JavaScript. Every object can have properties added or removed (unless [sealed](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/seal)). On the hand Rust structures are declared ahead of time. Rust *packs* field data together to build structures and turns keys into offsets in memory. The actual fields, the count, the names are all lost at runtime under regular conditions. On the other JavaScript objects at the surface level are all maps from keys (mostly strings...) to other JavaScript values. The names of properties is kept in memory at runtime. In Rust the rough equivalent type of a JS object would be: `HashMap<&'static str, Box<dyn std::any::Any>>` (ignoring object prototypes). You can use a `HashMap` to emulate the dynamic object from JS, on the whole 99% of Rust code uses consistent structures declared ahead of time.
## Our reflection example
Lets say we have a array of objects in JavaScript:
```javascript
const countries = [
	{ name: "Japan", population: 124_214_766, above_equator: true },
	{ name: "Mexico", population: 129_150_971, above_equator: true },
	{ name: "Australia", population: 26_020_300, above_equator: false },
	...
];
```
We want to store this list in string format, maybe to send over the wire or to store locally in the browser. A good option would be JSON, but let's say we want a more space efficient format as the every object in the list has the same keys so we don't need to serialize them for every item. We also have the assertion that the objects are shallow (not nested) with values of types numbers, strings and Booleans

We can write a function that:
- Encodes the length of the array
- Investigates the properties of the first objects (as the list is homogeneous (not heterogeneous), these facts apply to all objects in the array)
- Loops over items in the array
- Reads each field name in the object
- Based on the type, encodes the value into a low level representation
```javascript
const float32toString = (number) =>
	String.fromCharCode(...new Uint16Array(new Float32Array([number]).buffer));

function arrayToString(array) {
	if (array.length === 0) {
		return "";
	}

	const entries = Object.entries(array[0]);
	const fields = entries.map(([name, value]) => ({ name, ty: typeof value }));

	let buf = float32toString(array.length);

	for (const item of array) {
		for (const { name, ty } of fields) {
			const value = Reflect.get(item, name);
			if (value === null) {
				throw Error("Property value is null");
			}
			switch (ty) {
				case "string":
					const length = float32toString(new Blob([value]).size);
					buf += length + value;
					break;
				case "number":
					buf += float32toString(value);
					break;
				case "boolean":
					buf += value ? '\0' : '1';
					break;
			}
		}
	}
	return buf;
}
```

Here with `Object.entries` we can inspect the *shape* of an object at runtime. And using the `typeof` operator we can get the type of the value. (Again assuming that all objects have the same type):
```javascript
> Object.entries(countries[0])
[ [ "name", "Japan" ], [ "population", 124_214_766 ], [ "above_equator", true ] ]
> Object.entries(countries[0]).map(([name, value]) => ({ name, ty: typeof value }))
[
  { name: "name", ty: "string" },
  { name: "population", ty: "number" },
  { name: "above_equator", ty: "boolean" }
]
```

Our trick here is assuming that all the objects have the same properties (in technical documents this is referred to as a homogeneous array). This allows us to do the reflection once outside of the loop. Another fact is the fields are serialized in an the same order. If we did reflection on each we would have to be careful of retaining the order. Objects keys are in order of declaration, so can cause some problems with some of the reflection APIs:
```javascript
> Object.keys({a: 3, b: 2});
[ "a", "b" ]
> Object.keys({b: 3, a: 2});
[ "b", "a" ]
```

We also use `Reflect.get` to get a property under a given string key (`item[name]` is equivalent).

> The idea of the example is not to show how to do low-level byte conversion in JavaScript but to show how you can mix in the the introspection logic. As we will see later runtime reflection is very difficult to do in Rust as there are no equivalent `Object.entries`, `Reflect.get` functions or a `typeof` operator in the language.

Now we can serialize the array with `arrayToString(countries)`, we want a way to reverse the process! However deserialization process becomes a bit of a problem, reflection in JS can only be done when we have a existing structure in inspect. As there are no type/shape declarations in plain JavaScript, there is no reference of the shape of the object we want to deserialize our serialized string into.

If we were using JSON we would be okay as the keys are embed into the serialized format, in our example we don't save the keys. Instead we can send a representation array of the key type pairs we want the objects to look like. Using `fields` and with a bit of conversion from our low level formatted string we have the following:
```javascript
const stringToFloat32 = (string, offset) => {
	const u16 = new Uint16Array([
		string.charCodeAt(offset),
		string.charCodeAt(offset + 1),
	]).buffer;
	return new Float32Array(u16)[0];
};

function arrayFromString(string, fields) {
	let i = 0;
	const entries = stringToFloat32(string, i);
    i += 2;
	const array = [];
	for (let arrayIndex = 0; arrayIndex < entries; arrayIndex++) {
        const object = {};
		for (const { name, ty } of fields) {
			let value;
			switch (ty) {
				case "string":
					const length = stringToFloat32(string, i);
					i += 2;
					value = String.fromCharCode(
						...Array.from({ length }, (_, j) => string.charCodeAt(j + i)),
					);
					i += length;
					break;
				case "number":
					value = stringToFloat32(string, i);
					i += 2;
					break;
				case "boolean":
					value = string.charCodeAt(i) === 0;
					i++;
					break;
			}
			Reflect.set(object, name, value);
		}
        array.push(object);
	}
	return array;
}
```

Here we use another part of reflection `Reflect.set` which allows us to set a property of an existing object based on a string key (`name`).
### Problems with reflection in JavaScript
The `arrayFromString` function assumes that the caller has passed a standard array where every object has the same type. `Reflect.get` will fail at runtime as if the property doesn't not exist `null` will be returned and there is a catch to throw an `Error`, so nothing unsafe. The downside is the both the dynamic property lookup and null check is expensive at runtime. 

One way to catch property errors ahead of time is if we had used an extension to JS like TypeScript, which can type the parameters of the function and check the arguments of callers.
## Writing a Rust procedural macro
If you are not in a Rust project/workspace already you can create one with the `cargo new *name*` command. Before we start generating code we should declare a trait as a target for our macros output.
### The BinarySerializable trait
In our current create, we need to create a `trait` that will be describe the requirements for serializing and deserializing objects. It will be called `BinarySerializable`. Serialization will require adding data to a buffer. Deserialization will require pulling from an iterator (which iterates over bytes of a serialized buffer) and producing a term of `Self`. *We will assume the format is well formed and panic at runtime rather than proper handling with `Result` when deserializing.*
```rust
pub trait BinarySerializable {
    fn serialize(self, buf: &mut Vec<u8>);

    fn deserialize<I: Iterator<Item = u8>>(iter: &mut I) -> Self;
}
```

We can implement the `BinarySerializable` trait for the primitives that will be in our structures:
```rust
impl BinarySerializable for bool {
    fn serialize(self, buf: &mut Vec<u8>) {
        buf.push(self as u8)
    }

    fn deserialize<I: Iterator<Item = u8>>(iter: &mut I) -> Self {
        iter.next().unwrap() == (true as u8)
    }
}

impl BinarySerializable for u64 {
    fn serialize(self, buf: &mut Vec<u8>) {
        buf.extend_from_slice(&self.to_le_bytes());
    }

    fn deserialize<I: Iterator<Item = u8>>(iter: &mut I) -> Self {
        let mut buf = [0; u64::BITS as usize / 8];
        buf.fill_with(|| iter.next().unwrap());
        u64::from_le_bytes(buf)
    }
}

impl BinarySerializable for String {
    fn serialize(self, buf: &mut Vec<u8>) {
        (self.len() as u64).serialize(buf);
        buf.extend_from_slice(self.as_bytes());
    }

    fn deserialize<I: Iterator<Item = u8>>(iter: &mut I) -> Self {
        let length = u64::deserialize(iter) as usize;
        String::from_utf8(iter.take(length).collect()).unwrap()
    }
}
```

> Again this is how to article write not low level byte manipulation. But I know which language I prefer for doing this logic ðŸ˜„
### The problem
Now we have this, we want to add this to a definition for objects
```rust
struct Country {
	name: String,
	population: u64,
	above_equator: bool
}
```

We could implement `BinarySerializable` for `Country` writing manually in the following way:
```rust
impl BinarySerializable for Country {
    fn serialize(self, buf: &mut Vec<u8>) {
        self.name.serialize(buf);
        self.population.serialize(buf);
        self.above_equator.serialize(buf);
    }

    fn deserialize<I: Iterator<Item = u8>>(iter: &mut I) -> Self {
        Self {
            name: BinarySerializable::deserialize(iter),
            population: BinarySerializable::deserialize(iter),
            above_equator: BinarySerializable::deserialize(iter),
        }
    }
}
```

This is great and we have our desired functionality. In this code we have to be careful we serialize them and deserialize them in the same order. **Writing this `impl` block out for many structs with many fields would get tedious. If we add another `struct` we want to be serializable we don't want to have to have to copy the implementation over. If we change the definition of `Country` we don't want to have to amend this code.**
### Procedural macro time!
With some idea of the code we want to generate, we can now make Rust do it for us using a proc macro!

Rust procedural macros require there own special crate for their definition. This is because some of the constraints about how they are compiled. The unofficial Rust convention for derive macros is the name of trait or crate name + derive. So we will run the following from our current folder `cargo new --lib binary-serialize-derive`. With this crate we have to set it be a proc macro by setting the key in its `Cargo.toml`, we will also add dependencies [syn](https://crates.io/crates/syn) for parsing the contents of our structure and [quote](https://crates.io/crates/quote) for generating the output!
```toml
[package]
name = "binary-serialize-derive"
version = "0.1.0"
edition = "2021"

[lib]
# Important \/\/\/
proc-macro = true

[dependencies]
# Dependencies we use when writing the macro \/\/\/
quote = "1.0.23"
syn = "1.0.107"
```

In our macro we want to parse the input into a structure that we can read information from. In the below we can read `.fields` directly without having to understand what tokens refer to field names and such.
```rust
use proc_macro::TokenStream;
use quote::quote;
use syn::{parse_macro_input, DeriveInput, Data};

#[proc_macro_derive(BinarySerializable)]
pub fn my_macro(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);

    let name = input.ident;

    let Data::Struct(struct_data) = input.data else {
        unimplemented!("enums");
    };

    let serialize_fields = struct_data.fields.iter().map(|field| {
        let Some(ident) = &field.ident else {
            unimplemented!("tuple structs");
        };
        quote!( self.#ident.serialize(buf); )
    });

    let deserialize_fields = struct_data.fields.iter().map(|field| {
        let Some(ident) = &field.ident else {
            unimplemented!("tuple structs");
        };
        quote!( #ident: BinarySerializable::deserialize(iter) )
    });

    let expanded = quote! {
        impl BinarySerializable for #name {
            fn serialize(self, buf: &mut Vec<u8>) {
                #( #serialize_fields );*
            }
        
            fn deserialize<I: Iterator<Item = u8>>(iter: &mut I) -> Self {
                Self {
                    #( #deserialize_fields ),*
                }
            }
        }
    };

    TokenStream::from(expanded)
}
```

Our macro starts with a `pub` function with an attribute `#[proc_macro_derive(BinarySerializable)]` to show this is the derive macro for `BinarySerializable`. In the code we use two iterators. One that generates `serialize` calls on fields and one that deserializes fields and assign them to fields. Throughout we use `quote!` (which yes is another macro ðŸ¤¯). With quote we can describe the tokens we want to generate in a declarative form exactly the same as Rust's syntax. The `expanded` variable is code that is a copy of the above however using the *hash* `#` character we can specify variables we want to interpolate. For example our struct name is interpolated into `impl BinarySerializable for #name`. We then interpolate our iterators. Using parenthesis and asterisks we specify run through the items from the iterator with a separator in between. In the first case the serialize call statements are separated with semi-colons `#( #serialize_fields );*`.
## Using the procedural macro
With our macro written, we include our crate using a path dependency
```toml
[dependencies]
binary-serialize-derive = { path = "./binary-serialize-derive" }
```

Similar to a function or other items we can import and reference the macro. It does not clash as macros and types have different name-spaces therefore we can have a macro and trait with the same name in the scope. (traits exist in the type namespace). We reference that we want to generate code for `Country` using the `#[derive(...)` attribute on our struct (in the same way you may have implemented `Clone` from the standard library).

```rust
use binary_serialize_derive::BinarySerializable;

#[derive(BinarySerializable)]
struct Country {
    name: String,
    population: u64,
    above_equator: bool,
}
```

#### cargo expand-ing our macro (optional)
We can debug the output by installing [cargo-expand](https://github.com/dtolnay/cargo-expand) (`cargo install cargo-expand`). Running `cargo expand` we see the result which is the automatic generated what we wrote manually

![](/images/blog/cargo-expand-proc-macro-output.png)

> `cargo-expand` requires Rust nightly and can require running `cargo clean` if switching back and forth between stable.

Alternative adding `eprintln!("{}", expanded)` to the end  then running `cargo check` is also a good alternative, not as readable of output but also works for malformed token streams.

We can now emulate the what we were doing with JavaScript arrays using regular generics and trait logic in Rust:
```rust
pub fn serialize_list<T: BinarySerializable>(items: Vec<T>) -> Vec<u8> {
    let mut buf = Vec::new();
    buf.extend_from_slice(&items.len().to_le_bytes());
    for item in items {
        item.serialize(&mut buf);
    }
    buf
}
```

![](/images/blog/proc-macro-encoding-binary-output.png)

## Considerations
With procedural macros, as they are expanded at parse time there is no type information available to the macro. We can examine type reference syntactically but you can't rely on the characteristics as the following is valid Rust code:
```rust
type String = ();

struct X(String)
```

Additionally any macro can result in increased compile time. When building the crate all the macros need to be run and the output token streams need to be parsed. Be careful not to produce more tokens than is necessary.

Another problem is that the implementation has to be maintained across crates. When publishing you have to make sure that the crates are released at once and the main crate imports a macro who generates the most up to date code.
### syn-helpers
[syn-helpers](https://github.com/kaleidawave/syn-helpers) is a framework I have been working to abstract common derive patterns. Our short example works great for our `Country` struct, but building derive macros that work on `enum`s, items that have generics. It is still work in progress but should note that not all proc macros need long complex implementations.
### Inner annotations
Sometimes we want to have some custom behaviour for a field. Rust allows for writing attributes in lots of places. The attribute can be read from the syntax tree that syn parses and are simple to lookup. They can contain token stream arguments if even more information needs to be given to the macro. This is difficult to do in just JavaScript as there is no declarations to reference.

In our example if there was a field we didn't want to end up in the output and that could be generated at runtime using default. We could add the following attribute and in the logic of the macro doing some different handling here.
```rust
struct Country {
	// ...
	#[serialization_skip(using_default)]
	ignored_field: TypeThatImplementsDefault
}
```
Just remember when doing so to register with Rust that the attribute belongs to the derive macro by adding attributes here `#[proc_macro_derive(BinarySerializable, attributes(serialization_skip))]`
# Conclusion
## Performance characteristics
As code is generated at compile time. It can be faster than having to do the work at runtime. Compared to JavaScript reflection, the properties are known at compile time and we can read them using memory offsets. It is difficult to benchmark the difference between JavaScript's runtime reflection and Rust with proc macro compile time reflection, without the results including other differences in the languages.
## Real world procedural macro examples
Proc macros are used throughout Rust. Most standard library `derive`s are proc macro could be implemented as proc macros. [Serde](https://serde.rs/) a serialization and deserialization library which does a lot more advanced things than our example and for other formats such as JSON and YML uses derive macros.

Ezno, a compiler project I am working on, uses some bespoke ones in the AST definitions for generating logic for visiting/walking AST.
### Shuttle
Shuttle which is a Rust-native cloud development platform uses procedural macros to define how a service runs and what features it needs. It works a bit differently using *attribute* macros (as opposed to the *derive* macros we used in our example) which work on more items that just `structs` and `enum`s. They allow complete rewrites of token streams as opposed to just generating additional code. Here macros allow the defining how the service runs in the same place as the logic, rather than having to manage additional config files alongside. Long live infrastructure for code!
## In summary procedural macros are good for
- Common operations over structures, reducing boilerplate
- Performant reflection. Reduced worked compared to dynamic/runtime lookup
- Type checked, we can't run code that look at non-existent fields

Hopefully if you were previously perplexed, this post cleared up the why's and how's of procedural macros. If you make anything cool with Rust and need a place why not try Shuttle!

---

This blog post is powered by shuttle! If you have any questions, or want to provide feedback, join our [Discord server](https://discord.gg/shuttle)!

## [Shuttle](https://www.shuttle.rs/): The Rust-native, open source, cloud development platform.

Deploying and managing your Rust web apps can be an expensive, anxious and time consuming process.

If you want a batteries included and ops-free experience, [try out Shuttle](https://github.com/shuttle-hq/shuttle).

<hr styles="margin-top: 10px"/>